general:
  experiment: third_stage_fc
  profiler: False
  debug: False
  base_dir: "/export/compvis-nfs/user/daltmann/scratch/"
  seed: 42

# name of the pretrained first stage model (video autoencoding framework), the pretrained models are listed in the respective dictionary in models/pretrained_models.py .
# When having trained your own model, you can add it under a new key and specifiy its model name and the path to the checkpoint which shall be used
first_stage:
  name: iper-ss64-128
second_stage:
  name: iper-ss64-128
flow_encoder:
  name: iper-ss64-112

## name of the pretrained image encoder model (\phi_{x_0} in the paper), the pretrained models are listed in the respective dictionary in models/pretrained_models.py .
## When having trained your own model, you can add it under a new key and specifiy its model name and the path to the checkpoint which shall be used
#conditioner:
#  use: True
#  name: iper-ss64
#
## name of the pretrained poke embedder model (\phi_{c} in the paper), the pretrained models are listed in the respective dictionary in models/pretrained_models.py.
## When having trained your own model, you can add it under a new key and specifiy its model name and the path to the checkpoint which shall be used
#poke_embedder:
#  use: True
#  name: iper-ss64
#  #

data:
  dataset: IperDataset # supported datasets are IperDataset, PlantDataset, Human3.6mDataset and TaichiDataset
  # a window will be used for the poke with from (x-poke_size/2:x+poke_size/2,y-poke_size/2:y+poke_size/2) where (x,y) is the poked location.
  #All the values therein will be initialized with the respective pixel shift value
  poke_size: 5
  max_frames: 10 # number of predicted frames
  batch_size: 64
  n_workers: 10
  yield_videos: True # leave as is
  spatial_size: !!python/tuple [64,64] # spatial video resolution
  # data augmentation
  augment: True
  p_col: 0.8
  p_geom: 0.8
  augment_b: 0.4
  augment_c: 0.5
  augment_h: 0.15
  augment_s: 0.4
  aug_deg: 0
  aug_trans: !!python/tuple [0.1,0.1]
  split: official # see in the file data/flow_dataset.py
  n_pokes: 5 # the maximum number of pokes for a given training example. The actual number will be randomly chosen from within [1,n_pokes]
  zero_poke: False # whether or not to train with simulated zero pokes to force the model to learn foreground background separation
  zero_poke_amount: 12 #frequency, when zero pokes occur in the training (the amount of zeropokes per epoch is 1 / zero_poke_amount
  scale_poke_to_res: True # whether or not to scale the flow magnitudes according to the spatial downsampling of the videos
  filter: all # some datasets have special filter procedures, see data/flow_dataset.py

architecture:
  # flow_in_channels is taken from the second stage normalizing flow config
  flow_mid_channels: 1024
  flow_hidden_depth: 3
  n_flows: 8
  base_distribution: "normal"  # choose from {"radial", "normal"}


testing:
  n_samples_fid: 64
  # for diversity measure
  n_samples_per_data_point: 3
  test_batch_size: 128
#  n_samples_vis: 100
  n_samples_metrics: 1536
#  verbose: True
  debug: False
#  div_kp: False
#  summarize_n_pokes: False
  n_test_pokes: 5
  seed: 42
#  n_control_sensitivity_pokes: 32


training:
  lr: 1.0e-4
  weight_decay: 1.0e-5
  weight_recon: 64
  recon_scaling: False
  min_acc_batch_size: 3
  max_batches_per_epoch: 2000
  max_val_batches: 100
  use_logp_loss: False
  n_epochs: 100
  val_every: 0.8
  clip_grad_norm: 0.
  lr_scaling: True
  lr_scaling_max_it: 500
  custom_lr_decrease: False
  mixed_prec: False
  full_seq: True
  spatial_mean: False
  use_adabelief: False


logging:
  n_val_img_batches: 2
  log_train_prog_at: 200
  n_saved_ckpt: 3
  n_log_images: 8
  n_samples: 3
#  n_samples_umap: 1000
#  n_fvd_samples: 1000


#ui:
#  display_size: 256
#  debug: False
#  fixed_length: True
#  #seq_length_to_generate: 10
#  fps: 5
#  save_fps: 3
#  fixed_seed: False
#  interactive: False
#  ids: []
#  n_gt_pokes: 5
